param,value
env,None
task,lbf_reward_shaping
n_agents,2
observation_sizes,[10, 10]
action_sizes,[6, 6]
hidden_dim,128
shared_experience,False
shared_lambda,1.0
targets,simple
num_episodes,300000
max_episode_len,25
n_training_threads,1
gamma,0.99
tau,0.05
lr,0.0001
seed,None
steps_per_update,1
buffer_capacity,1000000
batch_size,128
epsilon,1.0
goal_epsilon,0.01
epsilon_decay,10
epsilon_anneal_slow,False
render,False
eval_frequency,50
eval_episodes,5
run,default
save_interval,100
training_returns_freq,100
env,None
agents,2
env_coef,0.2
oa_coef,1.0
discount_oa,0.8
ltl,(f & XF e)
field,5
decay_factor,0.9999992324719302
